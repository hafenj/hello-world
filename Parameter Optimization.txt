import tensorflow as tf
from tensorflow import keras 
from sklearn.model_selection import GridSearchCV 
from sklearn.metrics import confusion_matrix 
import numpy as np 
import matplotlib.pyplot as plt 
def load_data(): 
    '''Load the MNIST dataset''' 
     
    (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data() 
    return (X_train, y_train, X_test, y_test) 
  
#Task 1. load the dataset 
(X_train_2D, y_train, X_test_2D, y_test) = load_data() 
  
# 1.1 reshape the training and test sets (N*28*28) to N * 784. 10 points 
##  
X_train = np.reshape(X_train_2D, (60000, 784)) 
X_test = np.reshape(X_test_2D, (10000, 784)) 
print('X_train_2D: {}, X_train: {}'.format(X_train_2D.shape, X_train.shape)) 
print('X_test_2D: {}, X_test: {}'.format(X_test_2D.shape, X_test.shape)) 
## 
  
# 1.2 transform y_train to one-hot vectors using keras.utils.to_categorical to form the output of the NN. 10 points 
##  
y_train_onehot = keras.utils.to_categorical(y_train) 
print('y_train_onehot: {}'.format(y_train_onehot.shape)) 
print('one-hot vector examples:\n', y_train_onehot[:5]) 
## 
def create_NN(hidden_layers = [1000], act = 'relu', opt = 'rmsprop'):  
    '''create a deep feedforward neural network using keras 
     
    Parameters 
    ----------- 
    hidden_layers: a list that defines the numbers of hidden nodes for all hidden layers, e.g., [1000] indicates 
    the nn has only one hidden layer with 1000 nodes, while [1000, 500] defines two hidden layers and the first 
    layer has 1000 nodes and the second has 500 nodes. 
    act: activation function for all hidden layers 
    opt: optimizer 
     
    Returns 
    ------- 
    myNN: the neural network model 
     
    ''' 
    in_dim = 784 
    out_dim = 10 
     
    ##  
    myNN = keras.models.Sequential() 
     
    #2.1 build all hidden layers 
    for i in hidden_layers: 
        myNN.add(keras.layers.Dense( 
            units=i, # number of neurons, or cells 
            input_dim=in_dim, # Number of incoming nodes 
            kernel_initializer='glorot_uniform', 
            bias_initializer = 'zeros', 
            activation=act)) 
         
         
    #2.2 build the output layer and use the softmax activation 
    myNN.add(keras.layers.Dense( 
        units = y_train_onehot.shape[1], # number of neurons, or cells 
        input_dim = hidden_layers, 
        kernel_initializer = 'glorot_uniform', 
        bias_initializer = 'zeros', 
        activation = 'softmax')) 
  
     
    #2.3 choose the optimizer, compile the network and return it. Use 'accuracy' as the metrics 
    myNN.compile(optimizer=opt, loss='categorical_crossentropy', metrics = 'accuracy') 
     
    return myNN 
  
    ## 
     
h_nodes = [1000, 500] # two hidden layers with 1000 and 500 nodes, respectively. 
myNN = create_NN(hidden_layers= h_nodes, act = 'relu', opt = 'adam') 
myNN.summary() 
 
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier 
  
def nn_params_search(nn, X, y, param_grid): # 30 points 
    '''Search best paramaters 
    
    Parameters 
    ---------- 
    X_train: features 
    y_train: target of the input 
    param_grid: a dict that defines the parameters 
    
    Returns 
    ------- 
    best_params_ 
         
    ''' 
    ## cv = 3, scoring = 'accuracy', and verbose = 2 
    clf = GridSearchCV(estimator = nn, cv = 3, param_grid = param_grid , scoring = 'accuracy', verbose = 2) 
    clf.fit(X, y) 
    return(clf.best_params_) 
    
    ## 
  
# wrap keras model to use in sklearn 
nn = KerasClassifier(build_fn = create_NN, batch_size = 64, epochs = 10) # using the keras wapper 
param_grid = {'batch_size': [64, 128],  
              'act':['relu', 'sigmoid'], 'opt': ['sgd', 'adam']} 
  
best_params = nn_params_search(nn, X_train, y_train, param_grid = param_grid) 
print('\nBest parameters: ', best_params) 

def retrain_best_nn(best_params, X_train, y_train, epochs = 10): # 10 points 
    '''Retrain a nn using the best parameters 
     
    Paramters 
    ---------- 
    best_params: 
    X_train: data input of the training set 
    y_train: target of the input (one-hot vectors) 
     
    Returns 
    --------- 
    bestNN: the nn classifier trained using the best parameters 
     
    ''' 
    ##  
    best_NN = create_NN(act = best_params['act'], opt = best_params['opt']) 
    best_NN.fit(X_train, y_train, epochs = epochs, batch_size = best_params['batch_size']) 
    return(best_NN)     
     
    ## 
# 
bestNN = retrain_best_nn(best_params, X_train, y_train_onehot, epochs = 20) 